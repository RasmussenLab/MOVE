{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from move.data import io\n",
    "from move.tasks import encode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data\n",
    "\n",
    "This notebook runs part of the Multi-Omics Variational autoEncoder (MOVE) framework for using the structure the VAE has identified for extracting categorical data assositions across all continuous datasets. In the MOVE paper we used it for identifiying drug assosiations in clinical and multi-omics data. This part is a guide for encoding the data that can be used as input in MOVE.\n",
    "\n",
    "⚠️ The notebook takes user-defined configs in a `config/data` directory.\n",
    "\n",
    "For encoding the data you need to have each dataset in a TSV format. Each table has `N` &times; `M` shape, where `N` is the numer of samples/individuals and `M` is the number of features. The continuous data is z-score normalized, whereas the categorical data is one-hot encoded. Below is an example of processing a continuous and categorical datasets.\n",
    "\n",
    "First step is to read the configuration called `random_small` and specify the pre-defined task called `encode_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = io.read_config(\"random_small\", \"encode_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run the `encode_data` task, passing our `config` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO  - encode_data]: Beginning task: encode data\n",
      "[INFO  - encode_data]: Encoding 'random.small.drugs'\n",
      "[INFO  - encode_data]: Encoding 'random.small.proteomics'\n",
      "[INFO  - encode_data]: Encoding 'random.small.metagenomics'\n"
     ]
    }
   ],
   "source": [
    "encode_data(config.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be encoded accordingly and saved to the directory defined as `interim_data_path` in the `data` configuration.\n",
    "\n",
    "We can confirm how the data looks by loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(config.data.interim_data_path)\n",
    "\n",
    "cat_datasets, cat_names, con_datasets, con_names = io.load_preprocessed_data(path, config.data.categorical_names, config.data.continuous_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(cat_datasets) == 1  # one categorical dataset\n",
    "assert len(con_datasets) == 2  # two continuous datasets\n",
    "assert len(cat_names) == 1\n",
    "assert len(con_names) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drug dataset has been encoded as a matrix of 500 samples &times; 20 drugs &times; 2 categories (either took or did not take the drug), whereas the proteomics and metagenomics datasets keep their original shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random.small.drugs: (500, 20, 2)\n",
      "random.small.proteomics: (500, 200)\n",
      "random.small.metagenomics: (500, 1000)\n"
     ]
    }
   ],
   "source": [
    "dataset_names = config.data.categorical_names + config.data.continuous_names\n",
    "\n",
    "for dataset, dataset_name in zip(cat_datasets + con_datasets, dataset_names):\n",
    "    print(f\"{dataset_name}: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also confirm that the mean of the continuous datasets is now close to 0, and the standard deviation is close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random.small.proteomics: mean = -0.000, std = 0.975\n",
      "random.small.metagenomics: mean = 0.000, std = 0.975\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataset_name in zip(con_datasets, dataset_names[1:]):\n",
    "    print(f\"{dataset_name}: mean = {dataset.mean():.3f}, std = {dataset.std():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('move')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe194b1d27fbcd7437bf5eb8413313a2683e7f0cd626c1458cb32c6954f64d40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
