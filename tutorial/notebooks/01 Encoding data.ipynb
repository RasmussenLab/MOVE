{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from move.conf.schema import InputConfig\n",
    "from move.tasks.encode_data import EncodeData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data\n",
    "\n",
    "This notebook runs part of the Multi-Omics Variational autoEncoder (MOVE) framework for using the structure the VAE has identified for extracting categorical data assositions across all continuous datasets. In the MOVE paper we used it for identifiying drug assosiations in clinical and multi-omics data. This part is a guide for encoding the data that can be used as input in MOVE.\n",
    "\n",
    "⚠️ The notebook takes user-defined configs in a `config/data` directory.\n",
    "\n",
    "For encoding the data you need to have each dataset in a TSV format. Each table has `N` &times; `M` shape, where `N` is the numer of samples/individuals and `M` is the number of features. The continuous data is z-score normalized (`standardization`), whereas the categorical data is one-hot encoded (`one_hot_encoding`). Below is an example of processing a continuous and categorical datasets.\n",
    "\n",
    "First step is to locate where our `random_small` dataset is, which datasets it comprises, and what kind of pre-processing each dataset will undergo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../data\")\n",
    "interim_path = Path(\"../interim_data\")\n",
    "\n",
    "discrete_dnames = [\"random.small.drugs\"]\n",
    "continuous_dnames = [\"random.small.proteomics\", \"random.small.metagenomics\"]\n",
    "\n",
    "# Indicate which kind of pre-processing is required for each config file.\n",
    "# If a dataset has already been pre-processed, you can set this to 'none'\n",
    "disc_conf = [InputConfig(name, preprocessing=\"one_hot_encode\") for name in discrete_dnames]\n",
    "cont_conf = [InputConfig(name, preprocessing=\"standardize\") for name in continuous_dnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run the `EncodeData` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO  - EncodeData]: Beginning task: encode data\n",
      "[INFO  - EncodeData]: Encoding 'random.small.drugs'\n",
      "[INFO  - EncodeData]: Encoding 'random.small.proteomics'\n",
      "[INFO  - EncodeData]: Encoding 'random.small.metagenomics'\n"
     ]
    }
   ],
   "source": [
    "task = EncodeData(base_path, interim_path, \"random.small.ids\", disc_conf, cont_conf)\n",
    "task.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be encoded accordingly and saved to the directory defined as `interim_data_path` in the `data` configuration.\n",
    "\n",
    "We can confirm how the data looks by loading it and creating a `MoveDataset` object. This type of object concatenates our datasets and keeps the information such as original dataset shapes and feature names.\n",
    "\n",
    "The drug dataset has been encoded as a matrix of 500 samples &times; 20 drugs &times; 2 categories (either took or did not take the drug), whereas the proteomics and metagenomics datasets keep their original shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "            <thead>\n",
       "            <tr>\n",
       "                <th colspan=\"4\" style=\"text-align:center\">\n",
       "                    <abbr title=\"Multi-omics variational auto-encoders\"\n",
       "                    style=\"font-variant:small-caps; text-transform: lowercase\">\n",
       "                    MOVE</abbr> dataset (500 samples)\n",
       "                </th>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <th style=\"text-align:center\">data</th>\n",
       "                <th style=\"text-align:center\">type</th>\n",
       "                <th style=\"text-align:center\"># features</th>\n",
       "                <th style=\"text-align:center\"># classes</th>\n",
       "            </tr>\n",
       "            </thead>\n",
       "            <tbody><tr><td style='text-align:left'>random.small.drugs</td><td style='text-align:center'>discrete</td><td style='text-align:center'>20</td><td style='text-align:center'>2</td></tr><tr><td style='text-align:left'>random.small.proteomics</td><td style='text-align:center'>continuous</td><td style='text-align:center'>200</td><td style='text-align:center'>N/A</td></tr><tr><td style='text-align:left'>random.small.metagenomics</td><td style='text-align:center'>continuous</td><td style='text-align:center'>1,000</td><td style='text-align:center'>N/A</td></tr></tbody>\n",
       "        </table>"
      ],
      "text/plain": [
       "MoveDataset(3 datasets)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from move.data.dataset import MoveDataset\n",
    "\n",
    "dataset = MoveDataset.load(interim_path, discrete_dnames, continuous_dnames)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also confirm that the mean of the continuous datasets is now close to 0, and the standard deviation is close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random.small.proteomics: mean = -0.000, std = 0.975\n",
      "random.small.metagenomics: mean = 0.000, std = 0.975\n"
     ]
    }
   ],
   "source": [
    "for continuous_dataset in dataset.continuous_datasets:\n",
    "    print(f\"{continuous_dataset.name}: mean = {continuous_dataset.tensor.mean():.3f}, std = {continuous_dataset.tensor.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can directly read the config YAML and create a task object from its content.\n",
    "\n",
    "Note, that for this to work, the config directory structure must be in the same directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO  - EncodeData]: Beginning task: encode data\n",
      "[INFO  - EncodeData]: Encoding 'random.small.drugs'\n",
      "[INFO  - EncodeData]: Encoding 'random.small.proteomics'\n",
      "[INFO  - EncodeData]: Encoding 'random.small.metagenomics'\n"
     ]
    }
   ],
   "source": [
    "from move.data.io import read_config\n",
    "\n",
    "if not Path.cwd().joinpath(\"config/data\").exists():\n",
    "    raise FileNotFoundError(\"Requires a config files in the current working directory.\")\n",
    "\n",
    "config = read_config(\"random_small\", \"encode_data\", \"data.raw_data_path='../data'\")\n",
    "task = EncodeData.from_config(config)\n",
    "task.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('move')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e492c9f5b826854dfdf94b8d6b402bb809c46c7a6d638ce69ac84ffd4f448018"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
